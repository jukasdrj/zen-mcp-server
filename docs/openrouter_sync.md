# OpenRouter Model Sync Script

## Overview

The `scripts/sync_openrouter_models.py` script fetches the latest available models from OpenRouter's live API and updates the `conf/openrouter_models.json` configuration file. This keeps your OpenRouter models list current as new models are released.

## What It Does

1. **Fetches live models** from OpenRouter's `/models` endpoint
2. **Extracts capabilities** (context window, output tokens, vision support, etc.) from the API response
3. **Filters models** to include only stable, high-quality models from major providers
4. **Merges with curated data** - preserves your custom aliases, intelligence scores, and other metadata
5. **Generates updated config** with all models properly formatted

## Usage

### Basic Usage

```bash
python scripts/sync_openrouter_models.py
```

This fetches models from the public OpenRouter API endpoint (no auth required) and updates `conf/openrouter_models.json` with the latest models.

### Include OpenRouter Frontier Models

To include bleeding-edge OpenRouter-authored models (**Sonoma Dusk/Sky Alpha**, **Horizon Beta**, **Cypher Alpha**):

```bash
python scripts/sync_openrouter_models.py --include-frontier
```

These frontier models are prioritized with top intelligence scores (16-18) even when not yet in the public API.

### With Authentication

For higher rate limits and to see any private/custom models:

```bash
export OPENROUTER_API_KEY="your-api-key"
python scripts/sync_openrouter_models.py
```

### Preserve Custom Aliases

To keep your existing aliases while updating model data:

```bash
python scripts/sync_openrouter_models.py --keep-aliases
```

### Custom Output Path

```bash
python scripts/sync_openrouter_models.py --output /path/to/custom_models.json
```

## Model Filtering & Provider Strategy

### Excluded Providers

The script **explicitly excludes** models from providers available via native APIs:

- **OpenAI** - Use native OpenAI API (`conf/openai_models.json`) instead
- **Google** - Use native Gemini API (`conf/gemini_models.json`) instead
- **Anthropic** - Use native Claude API instead
- **X.AI** - Use native X.AI API (`conf/xai_models.json`) instead
- **Perplexity** - Lower priority specialty models
- **Free tier variants** (:free suffix models)

### Included Providers

Focuses on frontier, open-source, and specialized models:

**OpenRouter Frontier (Bleeding Edge)**:
- **Sonoma Dusk Alpha** (score: 17) - Latest frontier model
- **Horizon Beta** (score: 18) - Advanced frontier with large context
- **Sonoma Sky Alpha** (score: 16) - High-performance frontier
- **Cypher Alpha** (score: 16) - Specialized reasoning model
- *(Include with `--include-frontier` flag)*

**Frontier Specialists (Top Performance)**:
- **X.AI** - Grok-4, Grok Code (reasoning + coding specialists)
- **MiniMax** - 1M+ context frontier model
- **Qwen** (Alibaba - 38 models, excellent code specialists like Qwen3-Coder)
- **Z.AI/GLM** (Tsinghua - GLM 4.6 and reasoning models)

**Primary (Large/Capable)**:
- **Mistral** - Open alternative (Mistral Large)
- **Meta** - Llama 3.1, 405B largest open model
- **DeepSeek** - Advanced reasoning (R1)

**Secondary (Specialized)**:
- **Baidu** - Chinese LLM research
- **Tencent** - Enterprise/research models
- **ByteDance** - Advanced models
- **Microsoft** - Research models (Phi, etc.)
- **Cohere** - Specialized NLP
- **Nous Research** - Fine-tuned models
- **Moonshot** - Advanced reasoning
- **IBM Granite** - Enterprise models
- **NVIDIA** - Specialized models

## Intelligence Scoring

The script automatically assigns intelligence scores (1-20) to models based on OpenRouter metadata:

### Scoring Factors

- **Recent models** (+2 points) - Released in last 6 months
- **Context window** (+1 to +3 points):
  - 1M+ tokens: +3
  - 200K+ tokens: +2
  - 100K+ tokens: +1
- **Reasoning capability** (+3 points) - "reasoning", "R1", "pro" models
- **Model tier** (+2 or -1 points):
  - "70B", "405B", "large", "pro", "max": +2
  - "mini", "small", "lite": -1
- **Vision support** (+1 point)

### Score Range

- **1-5**: Small, specialized, or basic models
- **6-10**: Standard general-purpose models (majority)
- **11-15**: Advanced, large, or reasoning-capable models
- **16-20**: Frontier models (reserved for known top performers)

**Note**: Intelligence scores are generated by the sync script based on model metadata. They are **not** provided by OpenRouter. You can override individual scores by editing the config file manually.

## Curated Data Preservation

When the script runs with `--keep-aliases`, it preserves:

- **Custom aliases** - your short names for models (e.g., `deepseek-r1`, `mistral`)
- **Intelligence scores** - your manual quality ratings override the auto-generated ones
- **Capability overrides** - if you've manually set JSON mode, function calling, thinking mode, etc.

This means you can update the model list while keeping all your custom configuration and preferences.

## Output

The script logs:
- Number of models fetched from OpenRouter
- Number of models filtered out
- Number of final models included
- Success confirmation

Example:
```
2025-11-13 22:38:04,280 - INFO - Successfully fetched 344 models from OpenRouter
2025-11-13 22:38:04,284 - INFO - Filtered out 45 models, keeping 299
2025-11-13 22:38:04,286 - INFO - Updated config written to conf/openrouter_models.json
2025-11-13 22:38:04,286 - INFO - Total models: 299
2025-11-13 22:38:04,286 - INFO - ✓ Successfully synced OpenRouter models
```

## Current Model Coverage

The latest sync includes:

- **216 total models** from 49 providers (including 4 OpenRouter frontier models)
- **OpenRouter Frontier**: 4 bleeding-edge models (Sonoma Dusk/Sky, Horizon Beta, Cypher Alpha)
- **Qwen** (Alibaba): 38 models - Advanced Chinese LLM with code specialists
- **Mistral**: 31 models - Open alternative to frontier models
- **Meta Llama**: 16 models - Largest open-weight models (405B)
- **DeepSeek**: 13 models - Including R1 reasoning model
- **X.AI**: 7 models - Grok-4, Grok Code specialists
- **Microsoft**: 8 models - Phi and research models
- **Moonshot**: 6 models - Advanced reasoning models
- **Nous Research**: 6 models - Specialized fine-tuned models
- **MiniMax**: 3 models - 1M+ context frontier models
- **Z.AI/GLM**: Models from Tsinghua with reasoning
- Plus models from Baidu, Tencent, Amazon, IBM, NVIDIA, and others

**Explicitly excluded providers** (use native APIs instead):
- ~~OpenAI~~ → Use `openai_models.json`
- ~~Google~~ → Use `gemini_models.json`
- ~~Anthropic~~ → Use Anthropic API directly
- ~~X.AI~~ (native models only) → Use `xai_models.json` (OpenRouter versions still available)
- ~~Perplexity~~ → Lower priority

## Recommended Workflow

1. **After adding new models to OpenRouter or when their catalog updates:**
   ```bash
   python scripts/sync_openrouter_models.py --keep-aliases
   ```

2. **After major OpenRouter changes (quarterly check recommended):**
   ```bash
   python scripts/sync_openrouter_models.py
   ```

3. **Verify and test:**
   ```bash
   # Test that the server loads the new models correctly
   python -m pytest tests/test_listmodels.py -v

   # Test OpenRouter functionality
   python communication_simulator_test.py --individual test_openrouter_models
   ```

## Troubleshooting

### Network Issues

If you get network errors, check:
- Internet connectivity
- Firewall rules allowing HTTPS to `openrouter.ai`
- OpenRouter API status

### Rate Limiting

If you hit rate limits:
- Wait a few minutes
- Set `OPENROUTER_API_KEY` environment variable for higher limits
- Contact OpenRouter support for increased limits

### Models Not Updating

If models seem not to update:
- Check that the script completed successfully (look for "✓" message)
- Verify the output file was written: `ls -la conf/openrouter_models.json`
- Ensure you have write permissions in the `conf/` directory

## Implementation Details

The script uses Python's built-in `urllib` library for HTTP requests (no external dependencies). It parses the OpenRouter API response format:

```json
{
  "data": [
    {
      "id": "openai/gpt-5-pro",
      "name": "GPT-5 Pro",
      "description": "...",
      "context_length": 400000,
      "max_completion_tokens": 272000,
      "pricing": {...},
      "architecture": {...}
    }
  ]
}
```

And converts it to the Zen MCP Server config format with proper capability detection.

## Related Files

- `conf/openrouter_models.json` - Generated config file
- `providers/registries/openrouter.py` - OpenRouter registry that loads the config
- `providers/openrouter.py` - OpenRouter provider implementation
- `docs/custom_models.md` - General custom models documentation
